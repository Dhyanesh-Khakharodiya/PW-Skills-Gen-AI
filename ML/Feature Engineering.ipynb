{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1caf104d-825b-40c0-9afe-82a5558631bf",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6c5f2-76e1-43dc-8b9d-f3033aa3bf2c",
   "metadata": {},
   "source": [
    "1. What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb505e-5ce4-4b21-8df7-3908d0445f60",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "In Machine Learning, a *parameter* is a value that the model learns automatically from the training data during the training process. These values define how the model makes predictions.  \n",
    "  \n",
    "For example:  \n",
    "\n",
    "* In linear regression, the slope and intercept are parameters.\n",
    "\n",
    "* In neural networks, the weights and biases are parameters.  \n",
    "Parameters are adjusted during training to minimize error and improve the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f94c7-9817-4cc1-85e4-45785c9b34df",
   "metadata": {},
   "source": [
    "2. What is correlation?  \n",
    "What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8db465-46e8-4b48-b107-3ee27dff283d",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "*Correlation* is a statistical measure that describes the strength and direction of a relationship between two variables. It usually ranges from –1 to +1.\n",
    "  \n",
    "* +1 indicates a perfect positive relationship\n",
    "* 0 indicates no linear relationship\n",
    "* –1 indicates a perfect negative relationship\n",
    "  \n",
    "A negative correlation means that as one variable increases, the other variable decreases.  \n",
    "For example, if product price increases and demand decreases, this reflects a negative correlation between price and demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6c5eb-65b9-474b-a1fa-af5ba3fb1c09",
   "metadata": {},
   "source": [
    "3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2715f39-c09c-467a-a36f-d107ed95fac7",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "*Machine Learning* is defined as the field of study that gives computers the ability to learn from experience without being *explicitly programmed*.\n",
    "  \n",
    "A commonly used formal definition (by Tom Mitchell) states:  \n",
    "A computer program is said to learn from `Experience (E)` with respect to some `Task (T)` and `Performance measure (P)` if its performance at task T, as measured by P, improves with experience E.\n",
    "\n",
    "Main Components:  \n",
    "  \n",
    "* Task (T) – The specific problem the model is trying to solve.  \n",
    "Example: Classifying emails as spam or not spam.  \n",
    "  \n",
    "* Experience (E) – The data the model learns from.  \n",
    "Example: A dataset of labeled emails.  \n",
    "  \n",
    "* Performance (P) – The metric used to evaluate how well the model performs the task.\n",
    "Example: Accuracy, precision, recall, or error rate.  \n",
    "  \n",
    "In simple terms, machine learning involves improving performance on a task through experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a9e62-67ef-4b13-a6d4-148dfa78c097",
   "metadata": {},
   "source": [
    "4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2abedb-600b-40bd-86ef-f10e191a1062",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "The *loss value measures* how far the model’s predictions are from the actual target values. It quantifies the *error* made by the model.\n",
    "  \n",
    "* A lower loss value indicates that the predictions are closer to the true values, meaning the model is performing well.\n",
    "* A higher loss value indicates larger errors, meaning the model needs improvement.\n",
    "  \n",
    "During training, the objective is to minimize the loss function. If the loss consistently decreases over iterations, it shows that the model is learning effectively. However, to determine whether a model is truly good, loss should also be evaluated on validation or test data, not just training data, to ensure it generalizes well and is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf09f3-2ecb-4501-9d01-e3c9e6600959",
   "metadata": {},
   "source": [
    "5. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83261dc9-4b50-4cdf-851d-0cf5f01dcb06",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "*Continuous variables* are numerical variables that can take any value within a range, including decimals. They represent measurable quantities.\n",
    "  \n",
    "Examples:\n",
    "* Height\n",
    "* Weight\n",
    "* Temperature\n",
    "* Sales revenue\n",
    "  \n",
    "*Categorical variables* are variables that represent distinct groups or categories rather than numeric measurements.  \n",
    "  \n",
    "Examples:  \n",
    "* Gender\n",
    "* Product category\n",
    "* Payment method\n",
    "* Customer segment\n",
    "  \n",
    "In summary, continuous variables measure quantities, while categorical variables represent labels or groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fdd47-6c39-4c23-96e4-6e9f61d6e9f8",
   "metadata": {},
   "source": [
    "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7883c34-d187-43be-9b3e-ab9070934f45",
   "metadata": {},
   "source": [
    "Ans:     \n",
    "Categorical variables cannot be directly used in most machine learning algorithms because models require numerical input. Therefore, they must be converted into numerical form before training.\n",
    "  \n",
    "Common techniques to handle categorical variables:  \n",
    "\n",
    "1. **Label Encoding:**  \n",
    "* Each category is assigned a unique integer value.\n",
    "* Suitable for ordinal data (where order matters), such as Low, Medium, High.\n",
    "  \n",
    "2. **One-Hot Encoding:**  \n",
    "* Creates separate binary columns for each category.\n",
    "* Suitable for nominal data (no order), such as colors or cities.\n",
    "  \n",
    "3. **Ordinal Encoding:**  \n",
    "* Used when categories have a natural ranking.\n",
    "* Example: Education level (High School < Bachelor’s < Master’s).\n",
    "  \n",
    "4. **Target Encoding:**  \n",
    "* Replaces categories with the mean of the target variable for that category.\n",
    "* Often used in high-cardinality features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043f23e-ca10-449c-a9d7-15f00c7db92b",
   "metadata": {},
   "source": [
    "7. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0f78e-36a3-40ef-b0d8-24157c331a47",
   "metadata": {},
   "source": [
    "Ans:      \n",
    "Training and testing a dataset refers to splitting the available data into two parts to build and evaluate a machine learning model.\n",
    "  \n",
    "**Training Dataset:**  \n",
    "  \n",
    "* The training data is used to teach the model.\n",
    "* The model learns patterns, relationships, and parameters from this data.\n",
    "  \n",
    "**Testing Dataset:**  \n",
    "  \n",
    "* The testing data is used to evaluate how well the model performs on unseen data.\n",
    "* It helps determine whether the model generalizes well or is overfitting.\n",
    "  \n",
    "In simple terms, the model learns from the training set and is evaluated on the testing set to measure its real-world performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbed44-27c8-4002-ab4f-cde81e78ca80",
   "metadata": {},
   "source": [
    "8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e8f36-b68f-4a4d-9941-0c8ef0f8caf9",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "`sklearn.preprocessing` is a module in the Scikit-learn library used for data preprocessing and transformation before training machine learning models.\n",
    "\n",
    "It provides tools to:  \n",
    "\n",
    "* Scale numerical data (e.g., StandardScaler, MinMaxScaler)\n",
    "* Encode categorical variables (e.g., LabelEncoder, OneHotEncoder)\n",
    "* Normalize data\n",
    "* Binarize features\n",
    "\n",
    "Preprocessing is important because many machine learning algorithms perform better when the input data is properly scaled and formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48be27-52f9-4caa-80df-4c7a789c4e84",
   "metadata": {},
   "source": [
    "9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2ab6f-425b-47c0-8b5d-6dba7b57daa7",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "A test set is a portion of the dataset that is kept separate from the training data and used only to evaluate the performance of a trained model.\n",
    "  \n",
    "It contains unseen data, meaning the model has not learned from it during training. This helps measure how well the model generalizes to new, real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f59672-bbdd-4017-988e-f2ac32822ad1",
   "metadata": {},
   "source": [
    "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d2322-0a75-44ac-85ef-1238335812d7",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "In Python, we commonly use `train_test_split` from scikit-learn to divide the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb42057-d8b7-4bba-8103-721140712e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example features (X) and target (y)\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Split data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35468488-995e-4a2d-8577-eb4b1f869823",
   "metadata": {},
   "source": [
    "To approach a Machine Learning problem a structured approach typically includes:\n",
    "  \n",
    "1. Understand the Problem  \n",
    "Define the objective, business goal, and evaluation metric.  \n",
    "\n",
    "2. Collect and Explore Data (EDA)  \n",
    "Understand distributions, missing values, correlations, and patterns.  \n",
    "\n",
    "3. Data Preprocessing  \n",
    "Handle missing values, encode categorical variables, scale features, and split data.  \n",
    "\n",
    "4. Model Selection  \n",
    "Choose suitable algorithms based on the problem type (regression, classification, etc.).  \n",
    "  \n",
    "5. Model Training  \n",
    "Train the model using the training dataset.  \n",
    "\n",
    "6. Model Evaluation  \n",
    "Evaluate performance using appropriate metrics (accuracy, RMSE, precision, recall, etc.).  \n",
    "  \n",
    "7. Hyperparameter Tuning  \n",
    "Optimize model performance using techniques like Grid Search or Cross-Validation.  \n",
    "  \n",
    "8. Deployment and Monitoring  \n",
    "Deploy the model and monitor performance over time.  \n",
    "  \n",
    "This structured workflow ensures clarity, reproducibility, and effective model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ab059-5867-4ca6-8b1d-91e519e787f0",
   "metadata": {},
   "source": [
    "11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ba83b-f0af-4fe2-9b5b-3ad87de8144e",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Performing Exploratory Data Analysis (EDA) before fitting a model is important because it helps you understand the data before making assumptions.\n",
    "  \n",
    "EDA allows you to:  \n",
    "* Detect missing values, duplicates, and inconsistencies\n",
    "* Identify outliers that may affect model performance\n",
    "* Understand the distribution of variables\n",
    "* Examine relationships and correlations between features\n",
    "* Check for data imbalance in classification problems\n",
    "  \n",
    "Without EDA, you risk building a model on flawed or misunderstood data, which can lead to poor accuracy, biased results, or overfitting.  \n",
    "  \n",
    "In simple terms, EDA ensures the data is clean, meaningful, and suitable before applying any machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e38f43-d5e9-43a3-b718-fe529c8eeab5",
   "metadata": {},
   "source": [
    "12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af8de0-cc75-454c-b7ef-a5456c53b36f",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables.\n",
    "  \n",
    "It ranges from –1 to +1:  \n",
    "* +1 → Perfect positive correlation (both variables increase together)\n",
    "* 0 → No linear relationship\n",
    "* –1 → Perfect negative correlation (one increases while the other decreases)\n",
    "  \n",
    "Correlation helps understand how strongly two variables are related and whether the relationship is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb94534-02b6-4a21-9f5b-9bae45eb1da1",
   "metadata": {},
   "source": [
    "13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f1997-8965-45f7-b7f9-0449ad72bc14",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "A negative correlation means that as one variable increases, the other variable decreases.\n",
    "For example, if product price increases and demand decreases, this reflects a negative correlation between price and demand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec0e88-452e-4f95-b82d-0350f82abd91",
   "metadata": {},
   "source": [
    "14.  How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a28ea5-0507-40b1-bc22-cea3336d0af7",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "You can find correlation between variables in Python using NumPy or Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1edc62-2fd4-43b0-a6a6-e7125cf4441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.995893206467704\n"
     ]
    }
   ],
   "source": [
    "##using numpy\n",
    "import numpy as np\n",
    "\n",
    "x = [10, 20, 30, 40, 50]\n",
    "y = [15, 25, 35, 45, 60]\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0][1]\n",
    "print(\"Correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1cc3c9e-3249-4edb-a2e9-7a2b20adc4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y\n",
      "x  1.000000  0.995893\n",
      "y  0.995893  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"x\": [10, 20, 30, 40, 50],\n",
    "    \"y\": [15, 25, 35, 45, 60]\n",
    "})\n",
    "\n",
    "correlation = data.corr()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a770b-fe39-46b9-b432-8e96bce6b3ba",
   "metadata": {},
   "source": [
    "Both methods typically calculate the Pearson correlation coefficient by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8666781-815a-4e3e-bea3-60aa6f2477b0",
   "metadata": {},
   "source": [
    "15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a6819-c710-4c82-abfd-29c33dddae28",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "**Causation** means that one variable directly causes a change in another variable. There is a clear cause-and-effect relationship.\n",
    "\n",
    "Difference between Correlation and Causation  \n",
    "* Correlation means two variables move together, but it does not prove that one causes the other.\n",
    "* Causation means one variable directly influences the other.\n",
    "  \n",
    "Example:  \n",
    "Suppose data shows that ice cream sales and drowning incidents both increase during summer.  \n",
    "These two variables are positively correlated because they increase at the same time.  \n",
    "However, ice cream sales do not cause drowning.  \n",
    "  \n",
    "The actual cause is a third factor — hot weather — which increases both swimming activity (leading to drowning incidents) and ice cream consumption.\n",
    "\n",
    "Summary\n",
    "\n",
    "Correlation = Relationship between variables\n",
    "\n",
    "Causation = Direct cause-and-effect relationship\n",
    "\n",
    "**Correlation alone is not enough to conclude causation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c872e06-41e7-4781-bd37-b264e59b66c8",
   "metadata": {},
   "source": [
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dc6ec-286f-429e-ac81-41c44354dcfe",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "An optimizer is an algorithm used in machine learning and deep learning to adjust the model’s parameters (weights and biases) in order to minimize the loss function. It updates parameters step-by-step so that the model’s predictions become more accurate.\n",
    "  \n",
    "Common Types of Optimizers  \n",
    "1. Gradient Descent (Batch Gradient Descent):\n",
    "* This optimizer calculates the gradient using the entire training dataset before updating the parameters.\n",
    "* Stable but can be slow for large datasets.\n",
    "* Example: Linear regression training using full dataset at each step.\n",
    "\n",
    "2. Stochastic Gradient Descent (SGD):\n",
    "* SGD updates parameters using one training example at a time.\n",
    "* Faster updates.\n",
    "* More noisy but can escape local minima.\n",
    "* Example: Training a logistic regression model using individual samples.\n",
    "\n",
    "3. Mini-Batch Gradient Descent:\n",
    "* This is a combination of Batch and SGD. It updates parameters using small batches of data.\n",
    "* Faster and more stable.\n",
    "* Most commonly used in practice.\n",
    "* Example: Neural network training with batch size = 32.\n",
    "\n",
    "4. Momentum:\n",
    "* Momentum improves gradient descent by adding a fraction of the previous update to the current one.\n",
    "* Helps accelerate convergence.\n",
    "* Reduces oscillations.\n",
    "* Example: Deep neural network training with faster convergence.\n",
    "\n",
    "5. RMSProp\n",
    "* RMSProp adjusts the learning rate based on recent gradient magnitudes.\n",
    "* Works well for non-stationary problems.\n",
    "* Often used in recurrent neural networks.\n",
    "\n",
    "6. Adam (Adaptive Moment Estimation)\n",
    "* Adam combines Momentum and RMSProp.\n",
    "* Adapts learning rate automatically.\n",
    "* Fast convergence.\n",
    "* Most widely used optimizer in deep learning.\n",
    "* Example: Training convolutional neural networks in image classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8c83-d93d-4ce7-a3ed-6dc8e2c09835",
   "metadata": {},
   "source": [
    "17. What is sklearn.linear_model ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dce4e6-8afe-4492-8e2d-0383274c7a1b",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "`sklearn.linear_model` is a module in the Scikit-learn library that provides tools for implementing linear models used in regression and classification tasks.\n",
    "\n",
    "It includes algorithms such as:\n",
    "\n",
    "* Linear Regression – for predicting continuous values\n",
    "\n",
    "* Logistic Regression – for classification problems\n",
    "\n",
    "* Ridge and Lasso Regression – for regularization\n",
    "\n",
    "* ElasticNet – combination of Ridge and Lasso\n",
    " \n",
    "These models assume a linear relationship between input features and the target variable. The module is widely used because it is simple, efficient, and effective for many real-world machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2b515-c2b1-4656-9dc8-c0e65b37bb61",
   "metadata": {},
   "source": [
    "18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e54d5-26dc-4079-bdf7-3909ad15b9e1",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "`model.fit()` is used to train a machine learning model. It allows the model to learn patterns from the training data by adjusting its parameters to minimize the loss function.\n",
    "  \n",
    "In simple terms, it teaches the model using the provided data.  \n",
    "\n",
    "The required arguments are:\n",
    "* X → The input features (independent variables)  \n",
    "* y → The target variable (dependent variable)  \n",
    "  \n",
    "Some models may also accept additional optional arguments (such as sample weights), but X and y are the essential inputs for supervised learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e78757-e2a6-4422-92b8-6fb7a2d218ad",
   "metadata": {},
   "source": [
    "19. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24672635-a02f-43b0-b2db-2b5fe6aae832",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "`model.predict()` is used to generate predictions from a trained machine learning model. It applies the learned parameters to new input data and returns predicted values or class labels.\n",
    "  \n",
    "In simple terms, after training with model.fit(), we use model.predict() to make predictions on unseen data.  \n",
    "The required argument is:  \n",
    "  \n",
    "X → The input feature data for which predictions are to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb0119-a33d-44a2-b919-b504d273853b",
   "metadata": {},
   "source": [
    "20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20244d6c-ace0-4b97-8182-76e7cb6943c5",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "*Continuous variables* are numerical variables that can take any value within a range, including decimals. They represent measurable quantities.\n",
    "  \n",
    "Examples:\n",
    "* Height\n",
    "* Weight\n",
    "* Temperature\n",
    "* Sales revenue\n",
    "  \n",
    "*Categorical variables* are variables that represent distinct groups or categories rather than numeric measurements.  \n",
    "  \n",
    "Examples:  \n",
    "* Gender\n",
    "* Product category\n",
    "* Payment method\n",
    "* Customer segment\n",
    "  \n",
    "In summary, continuous variables measure quantities, while categorical variables represent labels or groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f181d-1f79-4baa-b18c-a494c59a8746",
   "metadata": {},
   "source": [
    "21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a1ef0-2a53-40cd-850f-96a2f3b20195",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "**Feature scaling** is the process of transforming numerical features so that they are on a similar scale or range.\n",
    "  \n",
    "In many datasets, features can have different ranges. For example, income may range in thousands, while age ranges between 18 and 60. Without scaling, features with larger values can dominate the model.\n",
    "  \n",
    "Common methods:  \n",
    "  \n",
    "1. Standardization (Z-score scaling) – centers data around mean = 0 and standard deviation = 1\n",
    "\n",
    "2. Min-Max scaling – rescales values between 0 and 1\n",
    "\n",
    "**How it helps in Machine Learning:**\n",
    "  \n",
    "* Improves model convergence speed\n",
    "\n",
    "* Prevents features with large ranges from dominating\n",
    "\n",
    "* Improves performance of distance-based algorithms (KNN, K-Means)\n",
    "\n",
    "* Essential for gradient-based algorithms (Linear Regression, Logistic Regression, Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550e930-22ab-48c4-bfc8-993ae78680a7",
   "metadata": {},
   "source": [
    "22. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fa914-4114-432d-8add-86a22b7eb056",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "We perform scaling in Python using the sklearn.preprocessing module from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8419f94c-8879-4626-9f1c-d812238076c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "##1. Standardization (Z-score Scaling)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[100], [200], [300], [400], [500]])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae8183-04c1-4fcc-8d77-efb540d26685",
   "metadata": {},
   "source": [
    "* Centers data around mean = 0\n",
    "* Scales to standard deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b39886f-7ed3-4308-890e-a7344a2990e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "##2. Min-Max Scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe431ee6-8385-4b15-a440-c88fe34f8b24",
   "metadata": {},
   "source": [
    "* Rescales values between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36994ea1-d1f9-4799-85c0-b8537cb0732b",
   "metadata": {},
   "source": [
    "Important Note:\n",
    "\n",
    "Always:\n",
    "\n",
    "Fit the scaler on training data\n",
    "\n",
    "Transform both training and testing data using the same scaler\n",
    "\n",
    "This prevents data leakage and ensures proper model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d4aa2-9540-4210-9fd7-c0402f678ef2",
   "metadata": {},
   "source": [
    "23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a95b9-3473-4f9b-a8ee-47faf7a9663a",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "`sklearn.preprocessing` is a module in the Scikit-learn library used for data preprocessing and transformation before training machine learning models.\n",
    "\n",
    "It provides tools to:  \n",
    "\n",
    "* Scale numerical data (e.g., StandardScaler, MinMaxScaler)\n",
    "* Encode categorical variables (e.g., LabelEncoder, OneHotEncoder)\n",
    "* Normalize data\n",
    "* Binarize features\n",
    "\n",
    "Preprocessing is important because many machine learning algorithms perform better when the input data is properly scaled and formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf698a-aaaa-4927-9429-dee4d0332f4f",
   "metadata": {},
   "source": [
    "24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c7561-3c01-4dfa-8889-b2cc441fd926",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "In Python, we commonly use `train_test_split` from scikit-learn to divide the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fb3aee-d379-4485-b73c-0294dad7d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example features (X) and target (y)\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Split data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a6f4a-b780-40d4-bf75-299396f29dff",
   "metadata": {},
   "source": [
    "25. Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beb605-a01e-4bf9-925b-a71ed4375f70",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Data encoding is the process of converting categorical variables into numerical form so they can be used in machine learning models. Most algorithms require numerical input, so categorical data must be transformed before training.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e8af1-39ee-4719-bcfa-dd2f5c09aeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
